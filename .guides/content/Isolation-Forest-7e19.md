### To begin, let's start talking about decision trees.

Many machine learning algorithms are not amenable to outliers. They tend to learn the inliers extremely well, but begin to fail once presented with data outside their norm.

The Isolation Forest algorithm is extremely good at identifying anomalous data points. Take a look at the figure on your right, which illustrates this algorithm: an Isolation forest is composed of many binary decision trees that attempt to separate data based on a decision function. Basically, this is fancy talk for trying to split data points into two subgroups based on some rule we define for the algorithm. However, when one subgroup is a leaf, meaning it cannot be broken down further, then we recognize that we have isolated a singular data point from the rest of the data! Depending on how easy this data point was to isolate, we then classify the point as an anomaly or not. Within non-anomalous points, there will be some points that appear to be outliers, but fall within a specific range that can be classified as normal (think: standard deviation).

As the goal of an Isolation Forest is to capture all the anomalous data within a dataset, we would greatly prefer to have false positives (identifying normal data as anomalous) than false negatives (identifying anomalous data as normal). This way, we can ensure that we capture most, if not all, of the anomalous data. The way you control this in the algorithm is by adjusting the contamination factor, which will allow you to specify how much of the dataset you're training an algorithm on you expect to be anomalous (if you expect 10%, the contamination factor should be set to 0.1).